---
title: "SMWA_Assignment_CPG1_2"
author: "Neeraj,Sahil,Amitesh,Amit,Kushal"
date: "09/06/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r version,include=TRUE}
# Check the r version
R.version
```

```{r setup2, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#Setup the environment
library(SnowballC)
library(tm)
library(ggplot2)
library(RColorBrewer)
library(wordcloud)
library(topicmodels)
library(data.table)
library(stringi)
library(qdap)
library(dplyr)
library(rJava)
library(syuzhet)
library(DT)
library(gridExtra)
library(grid)
library(sentimentr)
library(tidyr)
library(tidyselect)
library(tidytext)
library(sqldf)
```

#####Read Twitter Data

```{r tweets}
# Set directory and read data
tweets.df <- read.csv("BoycottChinaMAIN.csv")

# Convert char date to correct date format
tweets.df$created <- as.Date(tweets.df$created)
tweets.df$text <- as.character(tweets.df$text)
str(tweets.df)
dim(tweets.df)
summary(tweets.df)
```

#####Cleaning the text data by removing links, tags and delimiters.   
#####Build a Corpus, and specify the location to be the character Vectors  
```{r}

(n.tweet <- length(tweets.df))

# Create document corpus with tweet text and clean up
myCorpus<- Corpus(VectorSource(tweets.df$text)) 

#####convert to Lowercase
myCorpus <- tm_map(myCorpus,content_transformer(stri_trans_tolower))

#####Remove Stopwords                               
myStopWords<- c((stopwords('english')),c("rt","th","will","amp","t","s","unfor","may","can","uf","sov","b","p","ko","se","h","get","ufeeuff","hai","via","yet","aebakbaaz","shubhu"))
myCorpus<- tm_map(myCorpus,removeWords , myStopWords)


myCorpus <- tm_map(myCorpus,removeNumbers)
myCorpus <- tm_map(myCorpus,removePunctuation)
#####Remove Extra Whitespaces
myCorpus <- tm_map(myCorpus,stripWhitespace)

#remove links,urls
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
removePicURL <- function(x) gsub("pic.twitter.com[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))

#remove @usernames
removeUsername <- function(x) gsub("@[^[:space:]]*", "", x)  
myCorpus <- tm_map(myCorpus, content_transformer(removeUsername))

writeLines(strwrap(myCorpus[[1001]]$content,60))
writeLines(strwrap(myCorpus[[299]]$content,60))
writeLines(strwrap(myCorpus[[1783]]$content,60))


#####Remove anything except the english language(including nos.) and space
removeNumPunct <- function(x) gsub("[^[:alnum:][:space:]]*", "", x)   
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))

#####Remove Single letter words
removeSingle <- function(x) gsub(" . ", " ", x)   
myCorpus <- tm_map(myCorpus, content_transformer(removeSingle))

myCorpusCopy<- myCorpus

```


#####Find the terms used most frequently
```{r Term frequency}
dtm <- TermDocumentMatrix(myCorpus)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
datatable(head(d, 10))


tdm <- TermDocumentMatrix(myCorpus, control= list(wordLengths= c(1, Inf)))
tdm

(freq.terms <- findFreqTerms(tdm, lowfreq = 25))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq > 25)
df <- data.frame(term = names(term.freq), freq= term.freq)


(freq.terms <- findFreqTerms(tdm, lowfreq = 10))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq > 10)
df1 <- data.frame(term = names(term.freq), freq= term.freq)


(freq.terms <- findFreqTerms(tdm, lowfreq = 55))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq > 55)
df2 <- data.frame(term = names(term.freq), freq= term.freq)


(freq.terms <- findFreqTerms(tdm, lowfreq = 85))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq > 85)
df3 <- data.frame(term = names(term.freq), freq= term.freq)

```

#####plotting the graph of frequent terms
```{r Graph1}
p1=ggplot(df, aes(reorder(term, freq),freq)) + theme_bw() + geom_bar(stat = "identity")  + coord_flip() +labs(list(title="Term Frequency Chart @25", x="Terms", y="Term Counts")) 


p2=ggplot(df1, aes(reorder(term, freq),freq)) + theme_bw() + geom_bar(stat = "identity")  + coord_flip() +labs(list(title="Term Frequency Chart @10", x="Terms", y="Term Counts")) 


p3=ggplot(df2, aes(reorder(term, freq),freq)) + theme_bw() + geom_bar(stat = "identity")  + coord_flip() +labs(list(title="Term Frequency Chart @55", x="Terms", y="Term Counts")) 


p4=ggplot(df3, aes(reorder(term, freq),freq)) + theme_bw() + geom_bar(stat = "identity")  + coord_flip() +labs(list(title="Term Frequency Chart @85", x="Terms", y="Term Counts"))


grid.arrange(p1,p2,ncol=2)


```

#####plotting the word cloud
```{r Graph2,echo=TRUE}
grid.arrange(p3,p4,ncol=2)

#PLotting word frequencies
barplot(d[0:10,]$freq, las = 2, names.arg = d[0:10,]$word,
        col ="lightblue", main ="Most frequent words",
        ylab = "Word frequencies")


# Creating the wordcloud

word.freq <-sort(rowSums(as.matrix(tdm)), decreasing= F)
pal<- brewer.pal(8, "Dark2")
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 2, random.order = F, colors = pal, max.words = 200,c(2,.5))

```
##plotting positive word cloud
```{r}
ap_td <- tidy(tdm)
ap_sentiments <- ap_td %>%
  inner_join(get_sentiments("bing"), by = c(term = "word"))

ap_sentiments_positive<-ap_sentiments[ap_sentiments$sentiment=='positive',]
ap_sentiments_negative<-ap_sentiments[ap_sentiments$sentiment=='negative',]

plusve<-sqldf("select SUM(count) as Freq,term from ap_sentiments_positive group by term order by Freq")
negve<-sqldf("select SUM(count) as Freq,term from ap_sentiments_negative group by term order by Freq")

wordcloud(words = plusve$term,freq = plusve$Freq,min.freq = 2,random.order = F,colors = brewer.pal(8,"Dark2"),max.words = 200)
```
##plotting negative word cloud

```{r}
wordcloud(words = negve$term,freq = negve$Freq,min.freq = 2,random.order = F,colors = brewer.pal(8,"Dark2"),max.words = 200)
```





Find association with a specific keyword in the tweets - chinese, wangchuk, change, softwareinaweekhardwareinayear
```{r}
list1<- findAssocs(tdm, "chinese", 0.2)
corrdf1 <- t(data.frame(t(sapply(list1,c))))
corrdf1
```


```{r}
barplot(t(as.matrix(corrdf1)), beside=TRUE,xlab = "Words",ylab = "Corr",col = "blue",main = "Boycott China analysis - Chinese",border = "black",las=2)
```
```{r}
list1<- findAssocs(tdm, "wangchuk", 0.2)
corrdf1 <- t(data.frame(t(sapply(list1,c))))
corrdf1
```


```{r}
barplot(t(as.matrix(corrdf1)), beside=TRUE,xlab = "Words",ylab = "Corr",col = "blue",main = "Boycott China analysis - wangchuk",border = "black",las=2)
```


```{r}
list1<- findAssocs(tdm, "change", 0.2)
corrdf1 <- t(data.frame(t(sapply(list1,c))))
corrdf1
```


```{r}
barplot(t(as.matrix(corrdf1)), beside=TRUE,xlab = "Words",ylab = "Corr",col = "blue",main = "Boycott China analysis - change",border = "black",las=2)
```


```{r}
list1<- findAssocs(tdm, "softwareinaweekhardwareinayear", 0.2)
corrdf1 <- t(data.frame(t(sapply(list1,c))))
corrdf1
```


```{r}
barplot(t(as.matrix(corrdf1)), beside=TRUE,xlab = "Words",ylab = "Corr",col = "blue",main = "Boycott China analysis - softwareinaweekhardwareinayear",border = "black",las=2)
```


















```{r Graph3,echo=TRUE}
#Topic Modelling to identify latent/hidden topics using LDA technique
dtm <- as.DocumentTermMatrix(tdm)

rowTotals <- apply(dtm , 1, sum)

NullDocs <- dtm[rowTotals==0, ]
dtm   <- dtm[rowTotals> 0, ]

if (length(NullDocs$dimnames$Docs) > 0) {
tweets.df <- tweets.df[-as.numeric(NullDocs$dimnames$Docs),]
}

lda <- LDA(dtm, k = 7) # find 5 topic
term <- terms(lda, 7) # first 7 terms of every topic
(term <- apply(term, MARGIN = 2, paste, collapse = ", "))

```


```{r Graph4,echo=TRUE}
topics<- topics(lda)
topics<- data.frame(date=(tweets.df$created), topic = topics)
qplot (date, ..count.., data=topics, geom ="density",fill= term[topic],position="stack")+ theme(legend.title = element_text(colour="black", size=6)) + theme(legend.text = element_text(colour="black", size=6))
```


```{r Graph5,echo=TRUE}
#####Sentiment Analysis: understanding emotional valence in tweets using syuzhet

mysentiment<-get_nrc_sentiment((tweets.df$text))

# Get the sentiment score for each emotion
mysentiment.positive =sum(mysentiment$positive)
mysentiment.anger =sum(mysentiment$anger)
mysentiment.anticipation =sum(mysentiment$anticipation)
mysentiment.disgust =sum(mysentiment$disgust)
mysentiment.fear =sum(mysentiment$fear)
mysentiment.joy =sum(mysentiment$joy)
mysentiment.sadness =sum(mysentiment$sadness)
mysentiment.surprise =sum(mysentiment$surprise)
mysentiment.trust =sum(mysentiment$trust)
mysentiment.negative =sum(mysentiment$negative)

# Create the bar chart
yAxis <- c(mysentiment.positive,
           + mysentiment.anger,
           + mysentiment.anticipation,
           + mysentiment.disgust,
           + mysentiment.fear,
           + mysentiment.joy,
           + mysentiment.sadness,
           + mysentiment.surprise,
           + mysentiment.trust,
           + mysentiment.negative)

xAxis <- c("Positive","Anger","Anticipation","Disgust","Fear","Joy","Sadness",
           "Surprise","Trust","Negative")
colors <- c("green","red","blue","orange","red","green","orange","blue","green","red")
yRange <- range(0,yAxis)
barplot(yAxis, names.arg = xAxis, 
        xlab = "Emotional valence", ylab = "Score", main = "Twitter sentiment", 
        sub = "Formula 1 analysis", col = colors, border = "black", xpd = F, ylim = yRange,
        axisnames = T, cex.axis = 0.8, cex.sub = 0.8, col.sub = "blue",las=2)
```


```{r}
#####Sentiment Analysis : Plot by date - understanding cummulative sentiment score movement

mysentimentvalues <- data.frame(get_sentiment((tweets.df$text)))
colnames(mysentimentvalues)<-"polarity"
mysentimentvalues$date <- tweets.df$created


result <- aggregate(polarity ~ date, data = mysentimentvalues, sum)
result
plot(result, type = "l")

```


```{r}
#####Sentiment Analysis: Plot by date - understanding average sentiment score movement

result1 <- aggregate(polarity ~ date, data = mysentimentvalues, mean)
result1

plot(result1, type = "l")


```


```{r}
```
